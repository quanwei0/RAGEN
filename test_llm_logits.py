import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch.nn.functional as F

def test_llm_input_output_relationship():
    """æµ‹è¯•LLMä¸­input_idså’Œoutput.logitsä¹‹é—´çš„å…³ç³»"""
    
    # 1. åŠ è½½å°å‹æ¨¡å‹å’Œåˆ†è¯å™¨
    model_name = "gpt2"  # ä½¿ç”¨GPT-2ä½œä¸ºç¤ºä¾‹ï¼Œæ¯”è¾ƒå°
    print(f"åŠ è½½æ¨¡å‹: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # æ·»åŠ pad_tokenï¼ˆGPT-2é»˜è®¤æ²¡æœ‰ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # 2. å‡†å¤‡æµ‹è¯•æ–‡æœ¬
    texts = [
        "Hello, how are you?",
        "The weather is nice today.",
        "Machine learning is fascinating!"
    ]
    
    print("=" * 60)
    print("æµ‹è¯•æ–‡æœ¬:")
    for i, text in enumerate(texts):
        print(f"{i+1}. {text}")
    
    # 3. åˆ†è¯å¹¶åˆ›å»ºbatch
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=20)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]
    
    print("=" * 60)
    print("è¾“å…¥ç»´åº¦:")
    print(f"input_ids.shape: {input_ids.shape}")  # [batch_size, seq_len]
    print(f"attention_mask.shape: {attention_mask.shape}")
    
    # 4. æ¨¡å‹å‰å‘ä¼ æ’­
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
    
    print("=" * 60)
    print("è¾“å‡ºç»´åº¦:")
    print(f"output.logits.shape: {logits.shape}")  # [batch_size, seq_len, vocab_size]
    print(f"è¯æ±‡è¡¨å¤§å°: {logits.shape[-1]}")
    
    # 5. åˆ†ææ¯ä¸ªä½ç½®çš„é¢„æµ‹
    batch_size, seq_len, vocab_size = logits.shape
    
    print("=" * 60)
    print("è¯¦ç»†åˆ†æç¬¬ä¸€ä¸ªæ ·æœ¬:")
    print(f"åŸæ–‡: '{texts[0]}'")
    
    # æ˜¾ç¤ºtokens
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
    print("Tokens:", tokens)
    print("Token IDs:", input_ids[0].tolist())
    
    print("\næ¯ä¸ªä½ç½®çš„é¢„æµ‹åˆ†æ:")
    for pos in range(seq_len):
        if attention_mask[0, pos] == 1:  # åªåˆ†ææœ‰æ•ˆä½ç½®
            current_token = tokens[pos]
            current_id = input_ids[0, pos].item()
            
            # è·å–è¯¥ä½ç½®çš„logits
            position_logits = logits[0, pos]  # [vocab_size]
            
            # æ‰¾åˆ°æ¦‚ç‡æœ€é«˜çš„top-3é¢„æµ‹
            probs = F.softmax(position_logits, dim=-1)
            top_values, top_indices = torch.topk(probs, k=3)
            
            predicted_tokens = tokenizer.convert_ids_to_tokens(top_indices.tolist())
            
            print(f"ä½ç½® {pos}: å½“å‰token='{current_token}' (ID:{current_id})")
            print(f"  é¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„top-3:")
            for i, (token, prob) in enumerate(zip(predicted_tokens, top_values)):
                print(f"    {i+1}. '{token}' (æ¦‚ç‡: {prob:.4f})")
            print()

def test_critic_style_output():
    """æ¨¡æ‹ŸCriticç½‘ç»œçš„è¾“å‡ºï¼ˆæ¯ä¸ªä½ç½®è¾“å‡ºä¸€ä¸ªä»·å€¼ï¼‰"""
    print("=" * 60)
    print("æ¨¡æ‹ŸCriticç½‘ç»œè¾“å‡º:")
    
    # æ¨¡æ‹Ÿcriticç½‘ç»œçš„è¾“å‡ºå¤´ï¼švocab_size -> 1
    batch_size, seq_len, vocab_size = 2, 10, 1000
    
    # æ¨¡æ‹ŸåŸå§‹LLMè¾“å‡º
    mock_logits = torch.randn(batch_size, seq_len, vocab_size)
    print(f"åŸå§‹LLM logits: {mock_logits.shape}")
    
    # æ¨¡æ‹Ÿcritic head: çº¿æ€§å±‚å°†vocab_sizeç»´åº¦å‹ç¼©åˆ°1
    critic_head = torch.nn.Linear(vocab_size, 1)
    
    # Criticè¾“å‡ºï¼šæ¯ä¸ªä½ç½®ä¸€ä¸ªä»·å€¼
    critic_values = critic_head(mock_logits)  # [batch_size, seq_len, 1]
    critic_values = critic_values.squeeze(-1)  # [batch_size, seq_len]
    
    print(f"Criticä»·å€¼ä¼°è®¡: {critic_values.shape}")
    print(f"æ¯ä¸ªä½ç½®çš„ä»·å€¼: {critic_values[0]}")
    
    # æ¨¡æ‹Ÿresponseéƒ¨åˆ†æˆªå–
    response_length = 5
    response_values = critic_values[:, -response_length-1:-1]  # [batch_size, response_length]
    print(f"Responseéƒ¨åˆ†ä»·å€¼: {response_values.shape}")
    print(f"Responseä»·å€¼: {response_values[0]}")

if __name__ == "__main__":
    print("ğŸš€ æµ‹è¯•LLMè¾“å…¥è¾“å‡ºå…³ç³»")
    
    try:
        # æµ‹è¯•æ ‡å‡†è¯­è¨€æ¨¡å‹
        test_llm_input_output_relationship()
        
        # æµ‹è¯•criticé£æ ¼çš„è¾“å‡º
        test_critic_style_output()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…transformersåº“: pip install transformers torch") 